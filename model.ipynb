{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "toxic_train = pd.read_csv('data/train.csv')\n",
    "toxic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data\n",
    "toxic_train = toxic_train.loc[(toxic_train['toxic'] == 1) | \n",
    "                    (toxic_train['severe_toxic'] == 1)|\n",
    "                    (toxic_train['obscene'] == 1)|\n",
    "                    (toxic_train['threat'] == 1)|\n",
    "                    (toxic_train['insult'] == 1)|\n",
    "                    (toxic_train['identity_hate'] == 1)]\n",
    "try:\n",
    "    toxic_train = toxic_train.drop(['id'], axis=1)\n",
    "    toxic_train = toxic_train.to_numpy()\n",
    "except:\n",
    "    print('ID column already dropped.')\n",
    "finally:\n",
    "    toxic_data, toxic_classes = toxic_train[:,0], toxic_train[:,1:]\n",
    "toxic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize data\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(toxic_data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "for i, x in np.ndenumerate(toxic_data):\n",
    "    toxic_data[i] = np.array(text_pipeline(toxic_data[i]))\n",
    "\n",
    "toxic_data = np.array(toxic_data)\n",
    "toxic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import float64\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, classes):\n",
    "        self.data = data\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor((self.data[idx])), torch.tensor(self.classes[idx].astype(float64)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_layer =  nn.EmbeddingBag(vocab_size+1, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, 20, bidirectional=True) # nn.Tanh()\n",
    "        self.linear_1 = nn.Linear(40, 32) # nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(32, 16) # nn.ReLU()\n",
    "        self.linear_3 = nn.Linear(16, 6) # nn.Sigmoid()\n",
    "        self.t = nn.Tanh()\n",
    "        self.r = nn.ReLU()\n",
    "        self.s = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed_layer(x)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        out = self.t(out)\n",
    "        out = self.r(self.linear_1(out))\n",
    "        out = self.r(self.linear_2(out))\n",
    "        out = self.s(self.linear_3(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 64\n",
    "model = Model(VOCAB_SIZE, EMBED_DIM)\n",
    "dataset = CustomDataset(toxic_data, toxic_classes)\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "total_acc, total_count = 0, 0\n",
    "log_interval = 1600\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        l = loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if torch.all((predicted_label>0.5).float().eq(label)):\n",
    "            total_acc+=1\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f} | loss {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count, l\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'saved_model.pt')\n",
    "torch.save(vocab, 'saved_vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Toxic', 'Obscene'],\n",
       " tensor([9.8353e-01, 2.5800e-06, 8.9814e-01, 2.2236e-12, 1.3120e-03, 9.0744e-07],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"wagwan broski I love you\"\n",
    "def GetPrediction(text):\n",
    "    response = []\n",
    "    classes = ['Toxic','Severe Toxic', 'Obscene', 'Threat', 'Insult', 'Identity Hate']\n",
    "    text = text_pipeline(text)\n",
    "    text = torch.LongTensor(text)\n",
    "    text = text[None,:]\n",
    "    pred = model(text)[0]\n",
    "    results = (pred>0.8).nonzero()\n",
    "    for r in results:\n",
    "        response.append(classes[r[0].item()])\n",
    "    return response, pred\n",
    "\n",
    "GetPrediction(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.vocab.Vocab"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
