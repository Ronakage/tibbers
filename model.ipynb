{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data\n",
    "toxic_train = pd.read_csv('data/train.csv')\n",
    "toxic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       "       \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",\n",
       "       \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",\n",
       "       ...,\n",
       "       \"Your absurd edits \\n\\nYour absurd edits on great white shark was total vandalism and was very sexual. All you edit here is fucking bullshit like spam all over this useful encyclopedia so stop all your bullshit. The admins have you everywhere. The only choice for you is to stop this bullshit or else you'll be blocked permanently. User:Factual80man\",\n",
       "       '\"\\n\\nHey listen don\\'t you ever!!!! Delete my edits ever again I\\'m annoyed because the WWE 2K15 a few of the roster have been confirmed and your stupid ass deletes what I write. just stop!!!! Please STOP!!!! You don\\'t work 2k or WWE games so stop deleting other peoples shit if I get it wrong or others get it wrong let them they will get the hang of it eventually but don\\'t stick your most ass in their and I\\'m gonna delete the \"\"please do not insert the roster\"\" shit how do you not have it if has been confirmed!!!!! God your stupid.\"',\n",
       "       \"and i'm going to keep posting the stuff u deleted until this fucking site closes down have fun u stupid ass bitch don't ever delete anything fuckin hore like i said before go to hell\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean data\n",
    "toxic_train_c = toxic_train.loc[(toxic_train['toxic'] == 0) &\n",
    "                    (toxic_train['severe_toxic'] == 0)&\n",
    "                    (toxic_train['obscene'] == 0)&\n",
    "                    (toxic_train['threat'] == 0)&\n",
    "                    (toxic_train['insult'] == 0)&\n",
    "                    (toxic_train['identity_hate'] == 0)]\n",
    "\n",
    "toxic_train_t = toxic_train.loc[(toxic_train['toxic'] == 1) | \n",
    "                    (toxic_train['severe_toxic'] == 1)|\n",
    "                    (toxic_train['obscene'] == 1)|\n",
    "                    (toxic_train['threat'] == 1)|\n",
    "                    (toxic_train['insult'] == 1)|\n",
    "                    (toxic_train['identity_hate'] == 1)]\n",
    "\n",
    "frames = [toxic_train_c[:30000-toxic_train_t.shape[0]], toxic_train_t]\n",
    "toxic_train = pd.concat(frames)\n",
    "\n",
    "try:\n",
    "    toxic_train = toxic_train.drop(['id'], axis=1)\n",
    "    toxic_train = toxic_train.to_numpy()\n",
    "except:\n",
    "    print('ID column already dropped.')\n",
    "finally:\n",
    "    toxic_data, toxic_classes = toxic_train[:,0], toxic_train[:,1:]\n",
    "toxic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([  887,    77,     3,   144,   159,   268,    30,   890,  4270,\n",
       "              10073,  1252,   110,   433,    16,    60,  2111,     9,    26,\n",
       "               8930,     2,    55,  9089,    22,    81,  3761,   194,     6,\n",
       "               4334,    52,   170,  1285, 15376,  4082,     1,    10,    69,\n",
       "                 56,     9,    26,   314,     3,   565,    50,     3,    59,\n",
       "                 42,   192,     6,     9,    76,  3704,   101,     1,  2603,\n",
       "                  1,  3346,     1,  2492,     1,  1338])                    ,\n",
       "       array([  177,     9, 11526,     4,    54,  3128,    17,  1923,  6586,\n",
       "                  6,     9,    76,  4309,  2327,    32,     1,   135,     1,\n",
       "                 27,    59,    23,   990,  3099,     2,  1496,   898,     2,\n",
       "               8383,    27,   241,    23])                                  ,\n",
       "       array([ 238,  293,    2,    6,    9,   76,  137,   21,  265,    7,   99,\n",
       "               395,    1,   14,    9,   28,   55,   13,   17,  438,   12, 1779,\n",
       "               468,  603,  138,   10,  506,    7,   33,  386,  144,  384,   11,\n",
       "                30,   59,   42,    1,   54,  313,    7,  233,   88,   48,    3,\n",
       "              3157,  116,    3,  812,  573,    1])                             ,\n",
       "       ...,\n",
       "       array([   19,  2929,   144,    19,  2929,   144,    22,   372,   585,\n",
       "              14209,    36,  1045,   154,    10,    36,   139,   588,     1,\n",
       "                 46,     5,    99,    73,    12,    84,   202,    47,  1284,\n",
       "                 46,   164,    17,   773,   374,    45,   121,    46,    19,\n",
       "                202,     1,     3,   361,    25,     5,  3066,     1,     3,\n",
       "                103,  1803,    20,     5,    12,     7,   121,    17,   202,\n",
       "                 39,   325,     5,     9,   167,    24,   182,  3037,     1,\n",
       "                104, 13350])                                                ,\n",
       "       array([  238,   967,    56,     9,    26,     5,   257,     4,     4,\n",
       "                  4,     4,   258,    30,   144,   257,   128,     6,     9,\n",
       "                 76,  4346,    70,     3,  3049, 17466,     8,   357,    11,\n",
       "                  3, 10227,    25,    80,  1924,    10,    19,   149,    95,\n",
       "               3983,    43,     6,   345,     1,    55,   121,     4,     4,\n",
       "                  4,     4,    69,   121,     4,     4,     4,     4,     5,\n",
       "                 56,     9,    26,   187, 22203,    39,  3049,  1155,    45,\n",
       "                121,   453,    89,  1757,    71,    34,     6,    75,    14,\n",
       "                250,    39,   321,    75,    14,   250,   225,   114,    60,\n",
       "                 51,    75,     3,  3426,    11,    14,  2178,    40,    56,\n",
       "                  9,    26,  1048,    19,   181,    95,    15,   115,    10,\n",
       "                  6,     9,    76,   680,   258,     3,    69,    37,    21,\n",
       "               1728,     3, 10227,    71,    90,    37,     5,    21,    25,\n",
       "                 14,    34,    63,    80,  1924,     4,     4,     4,     4,\n",
       "                  4,   324,    19,   149,     1])                           ,\n",
       "       array([   10,     6,     9,    76,   142,     7,   217,  1056,     3,\n",
       "                472,    93,   189,   421,    17,    84,   298, 13073,   229,\n",
       "                 25,   666,    93,   149,    95,   129,    56,     9,    26,\n",
       "                257,   258,   211,   562, 10868,    47,     6,   169,   175,\n",
       "                 67,     7,   269])                                         ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorize data\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(toxic_data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "for i, x in np.ndenumerate(toxic_data):\n",
    "    toxic_data[i] = np.array(text_pipeline(toxic_data[i]))\n",
    "\n",
    "toxic_data = np.array(toxic_data)\n",
    "toxic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, classes):\n",
    "        self.data = data\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor((self.data[idx])), torch.tensor(self.classes[idx].astype(np.float64)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_layer =  nn.EmbeddingBag(vocab_size+1, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, 20, bidirectional=True) # nn.Tanh()\n",
    "        self.linear_1 = nn.Linear(40, 32) # nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(32, 16) # nn.ReLU()\n",
    "        self.linear_3 = nn.Linear(16, 6) # nn.Sigmoid()\n",
    "        self.t = nn.Tanh()\n",
    "        self.r = nn.ReLU()\n",
    "        self.s = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed_layer(x)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        out = self.t(out)\n",
    "        out = self.r(self.linear_1(out))\n",
    "        out = self.r(self.linear_2(out))\n",
    "        out = self.s(self.linear_3(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 64\n",
    "model = Model(VOCAB_SIZE, EMBED_DIM)\n",
    "dataset = CustomDataset(toxic_data, toxic_classes)\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |  3000/30000 batches | accuracy    0.452 | loss    0.895\n",
      "| epoch   0 |  6000/30000 batches | accuracy    0.528 | loss    0.202\n",
      "| epoch   0 |  9000/30000 batches | accuracy    0.566 | loss    0.023\n",
      "| epoch   0 | 12000/30000 batches | accuracy    0.567 | loss    0.058\n",
      "| epoch   0 | 15000/30000 batches | accuracy    0.593 | loss    0.761\n",
      "| epoch   0 | 18000/30000 batches | accuracy    0.568 | loss    0.387\n",
      "| epoch   0 | 21000/30000 batches | accuracy    0.602 | loss    0.011\n",
      "| epoch   0 | 24000/30000 batches | accuracy    0.617 | loss    0.053\n",
      "| epoch   0 | 27000/30000 batches | accuracy    0.591 | loss    0.037\n",
      "| epoch   1 |  3000/30000 batches | accuracy    0.624 | loss    0.015\n",
      "| epoch   1 |  6000/30000 batches | accuracy    0.622 | loss    0.014\n",
      "| epoch   1 |  9000/30000 batches | accuracy    0.635 | loss    0.006\n",
      "| epoch   1 | 12000/30000 batches | accuracy    0.647 | loss    0.002\n",
      "| epoch   1 | 15000/30000 batches | accuracy    0.653 | loss    0.175\n",
      "| epoch   1 | 18000/30000 batches | accuracy    0.654 | loss    0.284\n",
      "| epoch   1 | 21000/30000 batches | accuracy    0.639 | loss    0.243\n",
      "| epoch   1 | 24000/30000 batches | accuracy    0.631 | loss    0.195\n",
      "| epoch   1 | 27000/30000 batches | accuracy    0.633 | loss    0.001\n",
      "| epoch   2 |  3000/30000 batches | accuracy    0.647 | loss    0.144\n",
      "| epoch   2 |  6000/30000 batches | accuracy    0.672 | loss    0.000\n",
      "| epoch   2 |  9000/30000 batches | accuracy    0.679 | loss    0.534\n",
      "| epoch   2 | 12000/30000 batches | accuracy    0.659 | loss    0.057\n",
      "| epoch   2 | 15000/30000 batches | accuracy    0.672 | loss    0.070\n",
      "| epoch   2 | 18000/30000 batches | accuracy    0.670 | loss    0.641\n",
      "| epoch   2 | 21000/30000 batches | accuracy    0.663 | loss    0.003\n",
      "| epoch   2 | 24000/30000 batches | accuracy    0.664 | loss    0.100\n",
      "| epoch   2 | 27000/30000 batches | accuracy    0.659 | loss    0.000\n",
      "| epoch   3 |  3000/30000 batches | accuracy    0.691 | loss    0.026\n",
      "| epoch   3 |  6000/30000 batches | accuracy    0.701 | loss    0.000\n",
      "| epoch   3 |  9000/30000 batches | accuracy    0.697 | loss    0.000\n",
      "| epoch   3 | 12000/30000 batches | accuracy    0.701 | loss    0.155\n",
      "| epoch   3 | 15000/30000 batches | accuracy    0.701 | loss    0.002\n",
      "| epoch   3 | 18000/30000 batches | accuracy    0.700 | loss    0.000\n",
      "| epoch   3 | 21000/30000 batches | accuracy    0.699 | loss    0.014\n",
      "| epoch   3 | 24000/30000 batches | accuracy    0.687 | loss    0.000\n",
      "| epoch   3 | 27000/30000 batches | accuracy    0.694 | loss    0.000\n",
      "| epoch   4 |  3000/30000 batches | accuracy    0.711 | loss    0.851\n",
      "| epoch   4 |  6000/30000 batches | accuracy    0.736 | loss    0.000\n",
      "| epoch   4 |  9000/30000 batches | accuracy    0.737 | loss    0.134\n",
      "| epoch   4 | 12000/30000 batches | accuracy    0.721 | loss    0.000\n",
      "| epoch   4 | 15000/30000 batches | accuracy    0.724 | loss    0.260\n",
      "| epoch   4 | 18000/30000 batches | accuracy    0.732 | loss    0.000\n",
      "| epoch   4 | 21000/30000 batches | accuracy    0.717 | loss    0.103\n",
      "| epoch   4 | 24000/30000 batches | accuracy    0.718 | loss    0.140\n",
      "| epoch   4 | 27000/30000 batches | accuracy    0.729 | loss    0.002\n",
      "| epoch   5 |  3000/30000 batches | accuracy    0.735 | loss    0.000\n",
      "| epoch   5 |  6000/30000 batches | accuracy    0.768 | loss    0.310\n",
      "| epoch   5 |  9000/30000 batches | accuracy    0.778 | loss    0.000\n",
      "| epoch   5 | 12000/30000 batches | accuracy    0.754 | loss    0.148\n",
      "| epoch   5 | 15000/30000 batches | accuracy    0.753 | loss    0.050\n",
      "| epoch   5 | 18000/30000 batches | accuracy    0.753 | loss    0.134\n",
      "| epoch   5 | 21000/30000 batches | accuracy    0.750 | loss    0.061\n",
      "| epoch   5 | 24000/30000 batches | accuracy    0.749 | loss    0.011\n",
      "| epoch   5 | 27000/30000 batches | accuracy    0.751 | loss    0.000\n",
      "| epoch   6 |  3000/30000 batches | accuracy    0.762 | loss    0.000\n",
      "| epoch   6 |  6000/30000 batches | accuracy    0.781 | loss    0.025\n",
      "| epoch   6 |  9000/30000 batches | accuracy    0.790 | loss    0.000\n",
      "| epoch   6 | 12000/30000 batches | accuracy    0.783 | loss    0.163\n",
      "| epoch   6 | 15000/30000 batches | accuracy    0.783 | loss    0.000\n",
      "| epoch   6 | 18000/30000 batches | accuracy    0.775 | loss    0.000\n",
      "| epoch   6 | 21000/30000 batches | accuracy    0.791 | loss    0.026\n",
      "| epoch   6 | 24000/30000 batches | accuracy    0.783 | loss    0.004\n",
      "| epoch   6 | 27000/30000 batches | accuracy    0.775 | loss    0.000\n",
      "| epoch   7 |  3000/30000 batches | accuracy    0.798 | loss    0.026\n",
      "| epoch   7 |  6000/30000 batches | accuracy    0.820 | loss    0.008\n",
      "| epoch   7 |  9000/30000 batches | accuracy    0.813 | loss    0.016\n",
      "| epoch   7 | 12000/30000 batches | accuracy    0.799 | loss    0.186\n",
      "| epoch   7 | 15000/30000 batches | accuracy    0.805 | loss    0.226\n",
      "| epoch   7 | 18000/30000 batches | accuracy    0.809 | loss    0.000\n",
      "| epoch   7 | 21000/30000 batches | accuracy    0.807 | loss    0.094\n",
      "| epoch   7 | 24000/30000 batches | accuracy    0.817 | loss    0.019\n",
      "| epoch   7 | 27000/30000 batches | accuracy    0.812 | loss    0.000\n",
      "| epoch   8 |  3000/30000 batches | accuracy    0.817 | loss    0.052\n",
      "| epoch   8 |  6000/30000 batches | accuracy    0.847 | loss    0.000\n",
      "| epoch   8 |  9000/30000 batches | accuracy    0.834 | loss    0.000\n",
      "| epoch   8 | 12000/30000 batches | accuracy    0.830 | loss    0.027\n",
      "| epoch   8 | 15000/30000 batches | accuracy    0.834 | loss    0.316\n",
      "| epoch   8 | 18000/30000 batches | accuracy    0.817 | loss    0.126\n",
      "| epoch   8 | 21000/30000 batches | accuracy    0.826 | loss    0.010\n",
      "| epoch   8 | 24000/30000 batches | accuracy    0.835 | loss    0.473\n",
      "| epoch   8 | 27000/30000 batches | accuracy    0.814 | loss    0.000\n",
      "| epoch   9 |  3000/30000 batches | accuracy    0.848 | loss    0.000\n",
      "| epoch   9 |  6000/30000 batches | accuracy    0.865 | loss    0.000\n",
      "| epoch   9 |  9000/30000 batches | accuracy    0.857 | loss    0.000\n",
      "| epoch   9 | 12000/30000 batches | accuracy    0.849 | loss    0.025\n",
      "| epoch   9 | 15000/30000 batches | accuracy    0.853 | loss    0.000\n",
      "| epoch   9 | 18000/30000 batches | accuracy    0.850 | loss    0.077\n",
      "| epoch   9 | 21000/30000 batches | accuracy    0.842 | loss    0.000\n",
      "| epoch   9 | 24000/30000 batches | accuracy    0.844 | loss    0.066\n",
      "| epoch   9 | 27000/30000 batches | accuracy    0.827 | loss    0.155\n",
      "| epoch  10 |  3000/30000 batches | accuracy    0.864 | loss    0.000\n",
      "| epoch  10 |  6000/30000 batches | accuracy    0.880 | loss    0.000\n",
      "| epoch  10 |  9000/30000 batches | accuracy    0.884 | loss    0.000\n",
      "| epoch  10 | 12000/30000 batches | accuracy    0.864 | loss    0.043\n",
      "| epoch  10 | 15000/30000 batches | accuracy    0.881 | loss    0.044\n",
      "| epoch  10 | 18000/30000 batches | accuracy    0.869 | loss    0.001\n",
      "| epoch  10 | 21000/30000 batches | accuracy    0.875 | loss    0.000\n",
      "| epoch  10 | 24000/30000 batches | accuracy    0.865 | loss    0.000\n",
      "| epoch  10 | 27000/30000 batches | accuracy    0.849 | loss    0.029\n",
      "| epoch  11 |  3000/30000 batches | accuracy    0.877 | loss    0.004\n",
      "| epoch  11 |  6000/30000 batches | accuracy    0.886 | loss    0.232\n",
      "| epoch  11 |  9000/30000 batches | accuracy    0.891 | loss    0.110\n",
      "| epoch  11 | 12000/30000 batches | accuracy    0.889 | loss    0.195\n",
      "| epoch  11 | 15000/30000 batches | accuracy    0.887 | loss    0.228\n",
      "| epoch  11 | 18000/30000 batches | accuracy    0.886 | loss    0.000\n",
      "| epoch  11 | 21000/30000 batches | accuracy    0.889 | loss    0.004\n",
      "| epoch  11 | 24000/30000 batches | accuracy    0.888 | loss    0.000\n",
      "| epoch  11 | 27000/30000 batches | accuracy    0.883 | loss    0.000\n",
      "| epoch  12 |  3000/30000 batches | accuracy    0.890 | loss    0.015\n",
      "| epoch  12 |  6000/30000 batches | accuracy    0.913 | loss    0.004\n",
      "| epoch  12 |  9000/30000 batches | accuracy    0.906 | loss    0.000\n",
      "| epoch  12 | 12000/30000 batches | accuracy    0.907 | loss    0.000\n",
      "| epoch  12 | 15000/30000 batches | accuracy    0.907 | loss    0.000\n",
      "| epoch  12 | 18000/30000 batches | accuracy    0.895 | loss    0.240\n",
      "| epoch  12 | 21000/30000 batches | accuracy    0.885 | loss    0.000\n",
      "| epoch  12 | 24000/30000 batches | accuracy    0.892 | loss    0.000\n",
      "| epoch  12 | 27000/30000 batches | accuracy    0.893 | loss    0.012\n",
      "| epoch  13 |  3000/30000 batches | accuracy    0.907 | loss    0.000\n",
      "| epoch  13 |  6000/30000 batches | accuracy    0.918 | loss    0.000\n",
      "| epoch  13 |  9000/30000 batches | accuracy    0.902 | loss    0.000\n",
      "| epoch  13 | 12000/30000 batches | accuracy    0.911 | loss    0.000\n",
      "| epoch  13 | 15000/30000 batches | accuracy    0.912 | loss    0.065\n",
      "| epoch  13 | 18000/30000 batches | accuracy    0.912 | loss    0.000\n",
      "| epoch  13 | 21000/30000 batches | accuracy    0.904 | loss    0.006\n",
      "| epoch  13 | 24000/30000 batches | accuracy    0.909 | loss    0.000\n",
      "| epoch  13 | 27000/30000 batches | accuracy    0.900 | loss    0.000\n",
      "| epoch  14 |  3000/30000 batches | accuracy    0.917 | loss    0.000\n",
      "| epoch  14 |  6000/30000 batches | accuracy    0.920 | loss    0.013\n",
      "| epoch  14 |  9000/30000 batches | accuracy    0.912 | loss    0.076\n",
      "| epoch  14 | 12000/30000 batches | accuracy    0.919 | loss    0.220\n",
      "| epoch  14 | 15000/30000 batches | accuracy    0.919 | loss    0.032\n",
      "| epoch  14 | 18000/30000 batches | accuracy    0.929 | loss    0.002\n",
      "| epoch  14 | 21000/30000 batches | accuracy    0.919 | loss    0.000\n",
      "| epoch  14 | 24000/30000 batches | accuracy    0.920 | loss    0.045\n",
      "| epoch  14 | 27000/30000 batches | accuracy    0.912 | loss    0.000\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_acc, total_count = 0, 0\n",
    "log_interval = 3000\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        l = loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if torch.all((predicted_label>0.5).float().eq(label)):\n",
    "            total_acc+=1\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f} | loss {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count, l\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'saved_model.pt')\n",
    "torch.save(vocab, 'saved_vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Toxic'],\n",
       " tensor([9.9852e-01, 1.4484e-02, 5.4608e-05, 3.8599e-06, 2.2561e-03, 1.0767e-03],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"how can u be this fkn dumb lol?\"\n",
    "def GetPrediction(text):\n",
    "    response = []\n",
    "    classes = ['Toxic','Severe Toxic', 'Obscene', 'Threat', 'Insult', 'Identity Hate']\n",
    "    text = text_pipeline(text)\n",
    "    text = torch.LongTensor(text)\n",
    "    text = text[None,:]\n",
    "    pred = model(text)[0]\n",
    "    results = (pred>0.9).nonzero()\n",
    "    for r in results:\n",
    "        response.append(classes[r[0].item()])\n",
    "    return response, pred\n",
    "\n",
    "GetPrediction(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
